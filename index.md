# 深度学习视觉攻击

## 介绍

这是一个使用 python 和一些流行的库和框架，如 numpy, pandas, matplotlib, tensorflow, pytorch, gym 等，来实现对深度学习视觉模型的白盒和黑盒攻击的项目。

## 目的

这个项目的目的是学习和实践如何生成对抗样本，即那些经过微小修改就能欺骗深度学习模型，但是肉眼难以察觉的图像或视频。该项目还旨在评估不同攻击方法的有效性，如 FGSM, MI-FGSM, NI-FGSM 等，以及探索一些提高对抗样本转移性的新颖方法，如集成训练，数据增强，元学习等。

## 方法

该项目使用强化学习（DRL）作为深度学习视觉模型，并选择三个游戏环境（Pong, Cartpole, Breakout）作为视觉任务。该项目使用不同的 DRL 算法（Deep Q-Learning, Policy Gradients）来训练 DRL 代理，并使用白盒攻击方法（FGSM, MI-FGSM, NI-FGSM）来生成对抗样本，目的是测试对抗样本在不同算法和策略之间的转移性。该项目还尝试了一些提高对抗样本转移性的新颖方法，如使用更好的优化，集成训练，数据增强，元学习等。

## 结果

该项目显示了对抗样本可以成功地欺骗 DRL 代理，并降低其性能，以及不同攻击方法有不同的成功率，转移性和扰动大小。该项目还显示了一些新颖的方法可以提高对抗样本的转移性，但也有一些限制和挑战。

## 分析

该项目分析了不同攻击方法的优缺点，以及攻击结果的可能原因和影响。该项目还讨论了一些有趣的实际场景来应用攻击，如自动驾驶，无人机等，以及一些可能的缓解对抗攻击的方法，如对抗训练，检测等。

## 结论

该项目得出的结论是，深度学习视觉模型是容易受到对抗攻击的，而生成和转移对抗样本是一个具有挑战性和吸引力的话题。该项目还提出了一些未来的方向和开放性的问题，供进一步的研究和探索。
